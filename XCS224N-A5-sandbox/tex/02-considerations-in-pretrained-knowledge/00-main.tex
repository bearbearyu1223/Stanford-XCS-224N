\graphicspath{ {images/} }

% real numbers R symbol
% \newcommand{\Real}{\mathbb{R}}
% \newcommand{\Int}{\mathbb{Z}}

% encoder hidden
% \newcommand{\henc}{\bh^{\text{enc}}}
% \newcommand{\hencfw}[1]{\overrightarrow{\henc_{#1}}}
% \newcommand{\hencbw}[1]{\overleftarrow{\henc_{#1}}}

% encoder cell
% \newcommand{\cenc}{\bc^{\text{enc}}}
% \newcommand{\cencfw}[1]{\overrightarrow{\cenc_{#1}}}
% \newcommand{\cencbw}[1]{\overleftarrow{\cenc_{#1}}}

% decoder hidden
% \newcommand{\hdec}{\bh^{\text{dec}}}

% decoder cell
% \newcommand{\cdec}{\bc^{\text{dec}}}

% make it possible to copy/paste from code snippets without strange extra spaces or line numbers:
% https://tex.stackexchange.com/questions/4911/phantom-spaces-in-listings
\lstset{basicstyle=\ttfamily,columns=flexible,numbers=none}

\section{Considerations in pretrained knowledge}
% \begin{parts}

In this section, we are asking you the intuitions and considerations from the pretrained Transformer coding in the previous section.
We will create a pinned slack channel discussion for you to answer and discuss with the other learners.

These are not graded but we encourage you to participate the discussion of the following questions.

\begin{enumerate}[(a)]

% ISS
% \part[1]
\item
Succinctly explain why the pretrained (vanilla) model was able to achieve a higher accuracy than the accuracy of the non-pretrained.

\begin{answer}
% ### START CODE HERE ###
% ### END CODE HERE ###
\end{answer}

% ISS
% \part[2]
\item
Take a look at some of the correct predictions of the pretrain+finetuned vanilla model, as well as some of the errors.
We think you'll find that it's impossible to tell, just looking at the output, whether the model \textit{retrieved} the correct birth place, or \textit{made up} an incorrect birth place.
Consider the implications of this for user-facing systems that involve pretrained NLP components.
Come up with two reasons why this indeterminacy of model behavior may cause concern for such applications.

\begin{answer}
% ### START CODE HERE ###
% ### END CODE HERE ###
\end{answer}

% ISS
% \part[2]
\item
If your model didn't see a person's name at pretraining time, and that person was not seen at fine-tuning time either, it is not possible for it to have ``learned'' where they lived.
Yet, your model will produce \textit{something} as a predicted birth place for that person's name if asked.
Concisely describe a strategy your model might take for predicting a birth place for that person's name, and one reason why this should cause concern for the use of such applications.
\end{enumerate}

\begin{answer}
% ### START CODE HERE ###
% ### END CODE HERE ###
\end{answer}

% \end{parts}


 
